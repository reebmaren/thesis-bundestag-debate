{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b088ad",
   "metadata": {},
   "source": [
    "### Mallet LDA Topic Modeling\n",
    "+ proportions of topics in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028777d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dir instead of file\n",
    "\n",
    "# preprocessing: sorting individual speeches by year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8163db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of faction ids\n",
    "faction_ids = list(pd.read_csv('raw_data/factions.csv')['id'])\n",
    "print(faction_ids)\n",
    "print(len(faction_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of politician ids\n",
    "politician_ids = list(pd.read_csv('raw_data/politicians.csv')['id'])\n",
    "print(len(politician_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of years (not using pandas here because of file size)\n",
    "years = []\n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as fr:\n",
    "    head = next(fr)\n",
    "    for line in fr:\n",
    "        data = next(fr).strip().split(\"|\")\n",
    "        \n",
    "        years.append(data[11][:4])\n",
    "        \n",
    "# list of unique years\n",
    "years = list(sorted(set(years)))\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8462774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yearly folders\n",
    "os.mkdir(f'proc_data/individual_speeches')\n",
    "for year in years:\n",
    "    os.mkdir(f'proc_data/individual_speeches/{year}')\n",
    "    \n",
    "# load German module\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "    \n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as fr:\n",
    "    head = next(fr)\n",
    "    for line in fr:\n",
    "        data = line.strip().split(\"|\")\n",
    "        \n",
    "        faction = data[7]\n",
    "        sp_id = data[0] \n",
    "        pol = data[5]\n",
    "        year = data[11][:4]\n",
    "        speech = data[6].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        len_speech = len(speech.split())\n",
    "        \n",
    "        if len_speech > 150:\n",
    "            \n",
    "            doc = nlp(speech)\n",
    "        \n",
    "            # lemmatization excluding stopwords, punctuations and numbers\n",
    "            speech = [w.lemma_.lower() for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)]\n",
    "            # adding method lower()\n",
    "            \n",
    "            # removing punctuation\n",
    "            #speech = speech.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            #removing numbers\n",
    "            #speech = speech.translate(str.maketrans('', '', string.digits))\n",
    "\n",
    "            # putting everything back together\n",
    "            speech = ' '.join(speech)\n",
    "\n",
    "\n",
    "            if os.path.isdir(f'proc_data/individual_speeches/{year}'):\n",
    "\n",
    "                with open(f'proc_data/individual_speeches/{year}/{year}_{sp_id}_{pol}_{faction}.txt', 'w', encoding='UTF-8') as fw:\n",
    "                    fw.write(speech)\n",
    "                    fw.write('\\n')\n",
    "                    fw.write(' ')\n",
    "                    fw.write('\\n')\n",
    "\n",
    "            else:\n",
    "                os.mkdir(f'proc_data/individual_speeches/{year}')\n",
    "                with open(f'proc_data/individual_speeches/{year}/{year}_{sp_id}_{pol}_{faction}.txt', 'w', encoding='UTF-8') as fw:\n",
    "                    fw.write(speech)\n",
    "                    fw.write('\\n')\n",
    "                    fw.write(' ')\n",
    "                    fw.write('\\n')\n",
    "                    \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2fb95",
   "metadata": {},
   "source": [
    "### correcting lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years: \n",
    "\n",
    "    for filename in os.listdir(f'proc_data/individual_speeches/{year}'):\n",
    "        with open(os.path.join(f'proc_data/individual_speeches/{year}', filename), 'r', encoding='UTF-8') as f: # open in readonly mode\n",
    "          # do your stuff\n",
    "        \n",
    "            if not os.path.isdir(f'proc_data/individual/{year}'):\n",
    "\n",
    "                os.mkdir(f'proc_data/individual/{year}')\n",
    "            with open(f'proc_data/individual/{year}/{filename}', 'w', encoding='UTF-8') as g:\n",
    "\n",
    "                data_low = f.readline().lower()\n",
    "     \n",
    "                g.write(data_low)\n",
    "\n",
    "# used 'individual' for Mallet TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316872b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stop words from spaCy\n",
    "de = spacy.load('de_core_news_md')\n",
    "sw_spacy = de.Defaults.stop_words\n",
    "print(len(sw_spacy))\n",
    "print(sw_spacy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
