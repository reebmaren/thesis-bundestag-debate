{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabf2032",
   "metadata": {},
   "source": [
    "## Wordcounts\n",
    "+ one file per year with every speech in it \n",
    "+ every speech of that year in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a582d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as fr:\n",
    "    head = next(fr)\n",
    "    for line in fr:\n",
    "        data = line.strip().split(\"|\")\n",
    "\n",
    "        sp_id = data[0]\n",
    "        faction = data[7]\n",
    "        year = data[11][:4]\n",
    "        speech = data[6].lower()\n",
    "\n",
    "        speech = speech.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        #removing numbers\n",
    "        speech = speech.translate(str.maketrans('', '', string.digits))\n",
    "\n",
    "        doc = nlp(speech)\n",
    "\n",
    "        # lemmatization excluding stopwords, punctuations and numbers\n",
    "        speech = [w.lemma_.lower() for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)]\n",
    "        # added lower()\n",
    "        speech = ' '.join(speech)\n",
    "\n",
    "        try:\n",
    "            with open(f'proc_data/speeches/{year}.txt', 'a', encoding='UTF-8') as fw:\n",
    "                fw.write(speech)\n",
    "                #fw.write('\\n')\n",
    "                fw.write(' ')\n",
    "                #fw.write('\\n')\n",
    "        except:\n",
    "            with open(f'proc_data/speeches/{year}.txt', 'w', encoding='UTF-8') as fw:\n",
    "                fw.write(speech)\n",
    "                #fw.write('\\n')\n",
    "                fw.write(' ')\n",
    "                #fw.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0590d68",
   "metadata": {},
   "source": [
    "### Correcting lowercase in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years: \n",
    "\n",
    "        for filename in os.listdir(f'proc_data/speeches'):\n",
    "            with open(os.path.join(f'proc_data/speeches', filename), 'r', encoding='UTF-8') as f: # open in readonly mode\n",
    "\n",
    "                if not os.path.isdir(f'proc_data/speeches_new'):\n",
    "\n",
    "                    os.mkdir(f'proc_data/speeches_new')\n",
    "                with open(f'proc_data/speeches_new/{filename}', 'w', encoding='UTF-8') as g:\n",
    "\n",
    "                    #correcting lowercase before counter\n",
    "                    data_low = f.readline().lower()\n",
    "                    g.write(data_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520f6c5",
   "metadata": {},
   "source": [
    "+ count every word in line[0] \n",
    "+ safe count of every word in one year\n",
    "+ output: dict with counts of all words in one year for every year\n",
    "+ example: {'1949': {'damen': 1675, 'herren': 1238, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27102d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = defaultdict(lambda : defaultdict(int))  \n",
    "for year in years: \n",
    "    \n",
    "    with open(f'proc_data/speeches_new/{year}.txt', 'r', encoding='UTF-8') as fr:\n",
    "\n",
    "            data_lower = fr.readlines()[0].lower()\n",
    "            data = data_lower.split()\n",
    "            \n",
    "            z = Counter(data)\n",
    "            z = dict(z)\n",
    "            wordcount[str(year)] = z \n",
    "\n",
    "    wordcount = dict(wordcount)\n",
    "\n",
    "    with open('proc_data/count_speeches/count_new.pickle', 'wb') as handle:  \n",
    "\n",
    "        pickle.dump(wordcount, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f96a6e",
   "metadata": {},
   "source": [
    "### Total wordcounts per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ab533",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordcnt = {}\n",
    "for year in wordcnt:\n",
    "    year_counts = wordcnt[year]\n",
    "    total_words = sum(year_counts.values())\n",
    "    new_wordcnt[year] = total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proc_data/count_speeches/total_wordcnt.pickle', 'wb') as handle:  \n",
    "\n",
    "        pickle.dump(new_wordcnt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135fc28",
   "metadata": {},
   "source": [
    "### Wordcount for keywords\n",
    "+ open count_new pickle file\n",
    "+ loop through counts for keywords, append to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_einwanderung = []\n",
    "\n",
    "with open('proc_data/count_speeches/count_new.pickle', 'rb') as handle:  \n",
    "\n",
    "    wordcnt = pickle.load(handle)\n",
    "    \n",
    "    for year,counting in wordcnt.items():\n",
    "        try:\n",
    "            cnt_einwanderung.append(counting['einwanderung'])\n",
    "        except:\n",
    "           cnt_einwanderung.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a60c23",
   "metadata": {},
   "source": [
    "### Wordcount filtered by faction\n",
    "+ filter speeches: one file for every year\n",
    "+ filter by faction\n",
    "+ wordcount keywords by year by party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "# CDU/CSU = 4\n",
    "# SPD = 23\n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as fr:\n",
    "    head = next(fr)\n",
    "    for line in fr:\n",
    "        data = line.strip().split(\"|\")\n",
    "\n",
    "        faction = data[7]\n",
    "        year = data[11][:4]\n",
    "        speech = data[6].lower()\n",
    "\n",
    "        if (faction == str(4) or faction == str(23)): \n",
    "            \n",
    "            # removing punctuation\n",
    "            speech = speech.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "            #removing numbers\n",
    "            speech = speech.translate(str.maketrans('', '', string.digits))\n",
    "            doc = nlp(speech)\n",
    "            \n",
    "            # lemmatization excluding stopwords, punctuations and numbers\n",
    "            speech = [w.lemma_.lower() for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)]\n",
    "            speech = ' '.join(speech)\n",
    "            \n",
    "            if os.path.isdir(f'proc_data/faction/{faction}'):\n",
    "            #try:\n",
    "                with open(f'proc_data/faction/{faction}/{faction}_{year}.txt', 'w', encoding='UTF-8') as fw:\n",
    "                    fw.write(speech)\n",
    "                    #fw.write('\\n')\n",
    "                    fw.write(' ')\n",
    "                    #fw.write('\\n')\n",
    "            #except:\n",
    "            else:\n",
    "                os.mkdir(f'proc_data/faction/{faction}')\n",
    "            \n",
    "                with open(f'proc_data/faction/{faction}/{faction}_{year}.txt', 'w', encoding='UTF-8') as fw:\n",
    "                    fw.write(speech)\n",
    "                    #fw.write('\\n')\n",
    "                    fw.write(' ')\n",
    "                    #fw.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91b1db",
   "metadata": {},
   "source": [
    "### correcting lowercase in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:  \n",
    "\n",
    "    if (faction == str(4)): #or faction == str(23)):\n",
    "        for filename in os.listdir(f'proc_data/faction_speeches/4'):\n",
    "            with open(os.path.join(f'proc_data/faction_speeches/4', filename), 'r', encoding='UTF-8') as f: # open in readonly mode\n",
    "              # do your stuff\n",
    "\n",
    "                if not os.path.isdir(f'proc_data/faction/{faction}'):\n",
    "\n",
    "                    os.mkdir(f'proc_data/faction/{faction}')\n",
    "                with open(f'proc_data/faction/{faction}/{filename}', 'w', encoding='UTF-8') as g:\n",
    "\n",
    "                    data_low = f.readline().lower()\n",
    "                    \n",
    "                    g.write(data_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025c641",
   "metadata": {},
   "source": [
    "+ every speech in one line\n",
    "+ count words in line[0]\n",
    "+ safe count of every word in one year by faction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e253ce",
   "metadata": {},
   "source": [
    "### Wordcount file CDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fec54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDU/CSU\n",
    "faction = str(4)\n",
    "wordcnt = defaultdict(lambda : defaultdict(int))  \n",
    "for year in years: \n",
    "    with open(f'proc_data/faction_speeches/{faction}/{faction}_{year}.txt', 'r', encoding='UTF-8') as fr:\n",
    "        \n",
    "        #correcting lowercase before counter\n",
    "        data_low = fr.readlines()[0].lower()\n",
    "        data = data_low.split()\n",
    "\n",
    "        z = Counter(data)\n",
    "        z = dict(z)\n",
    "        wordcnt[str(year)] = z \n",
    "        \n",
    "wordcnt = dict(wordcnt)\n",
    "# wordcnt_CDU\n",
    "with open('proc_data/count_speeches/count_faction.pickle', 'wb') as handle:  \n",
    "\n",
    "    pickle.dump(wordcnt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31908fc1",
   "metadata": {},
   "source": [
    "### Wordcount file SPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b82dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPD\n",
    "faction = str(23)\n",
    "wordcnt_SPD = defaultdict(lambda : defaultdict(int))  \n",
    "for year in years: \n",
    "    with open(f'proc_data/faction_speeches/{faction}/{faction}_{year}.txt', 'r', encoding='UTF-8') as file:\n",
    "\n",
    "        data_low = file.readlines()[0].lower()\n",
    "        data = data_low.split()\n",
    "\n",
    "        z = Counter(data)\n",
    "        z = dict(z)\n",
    "        wordcnt_SPD[str(year)] = z \n",
    "        \n",
    "wordcnt_SPD = dict(wordcnt_SPD)\n",
    "\n",
    "with open('proc_data/count_speeches/count_faction_23.pickle', 'wb') as handle:  \n",
    "\n",
    "    pickle.dump(wordcnt_SPD, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052488eb",
   "metadata": {},
   "source": [
    "### Example Output Wordcount by faction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with the counting per speech per year per faction (from dict)\n",
    "cnt_einwanderung_SPD = []\n",
    "\n",
    "with open('proc_data/count_speeches/count_faction_23.pickle', 'rb') as handle:  \n",
    "\n",
    "    freqcnt = pickle.load(handle)\n",
    "    \n",
    "    for year,counting in freqcnt.items():\n",
    "        try:\n",
    "            cnt_einwanderung_SPD.append(counting['einwanderung'])\n",
    "        except:\n",
    "            cnt_einwanderung_SPD.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e50d9",
   "metadata": {},
   "source": [
    "### Example Visualisation\n",
    "+ plotting counts per year\n",
    "+ Stacked bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4430bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as fr:\n",
    "    head = next(fr)\n",
    "    for line in fr:\n",
    "        data = next(fr).strip().split(\"|\")\n",
    "        \n",
    "        years.append(data[11][:4])\n",
    "        \n",
    "years = list(sorted(set(years)))\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e318d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(item) for item in years]\n",
    "word2_all = cnt_einwanderung\n",
    "width = 0.80\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (8, 6))\n",
    "\n",
    "ax.bar(labels, word2_all, width, label='all', color='purple')\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('term frequency')\n",
    "ax.set_title('Term frequency \"einwanderung\"')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb88a4",
   "metadata": {},
   "source": [
    "+ example visualisation wordcount by faction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(item) for item in years]\n",
    "cnt_all = cnt_einwanderung\n",
    "spd = cnt_einwanderung_SPD\n",
    "width = 0.80\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (8, 6))\n",
    "\n",
    "ax.bar(labels, cnt_all, width, label='all', color='purple')#, bottom=spd)\n",
    "ax.bar(labels, spd, width, label='SPD', color='red')\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('term frequency')\n",
    "ax.set_title('Term frequency \"einwanderung\" by year and faction')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e8f40",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530650a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "#keywords = ['ausländer', 'einwanderung', 'einbürgerung', 'asylbewerber', 'integration', 'kulturell', 'ausländisch']\n",
    "keywords = ['multikulti', 'leitkultur']\n",
    "#frequencies = defaultdict(lambda: defaultdict(int))\n",
    "frequencies_new = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "def lemmatize(doc):\n",
    "    return [w.lemma_.lower() for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)]\n",
    "\n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as f:\n",
    "   \n",
    "    headers = next(f) \n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        \n",
    "        data = line.strip().split(\"|\")\n",
    "        \n",
    "        faction = data[7]\n",
    "        sp_id = data[0] \n",
    "        pol = data[5]\n",
    "        year = data[11][:4]\n",
    "        speech = data[6].lower()\n",
    "\n",
    "        len_speech = len(speech.split())\n",
    "\n",
    "        if len_speech > 150:\n",
    "            doc = nlp(speech)\n",
    "            speech = lemmatize(doc)\n",
    "            \n",
    "            \n",
    "            for keyword in keywords:\n",
    "                    count = speech.count(keyword)\n",
    "                    #frequencies[sp_id][keyword] += count\n",
    "                    frequencies_new[sp_id][keyword] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ab789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freqcnt = dict(frequencies)\n",
    "freqcnt_new = dict(frequencies_new)\n",
    "#with open('proc_data/count_speeches/freq_count.pickle', 'wb') as handle:  \n",
    "with open('proc_data/count_speeches/freq_count_new.pickle', 'wb') as handle:  \n",
    "    #pickle.dump(freqcnt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(freqcnt_new, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
