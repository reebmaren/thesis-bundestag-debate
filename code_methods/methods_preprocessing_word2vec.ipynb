{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8790d48d",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "+ manipulating the data\n",
    "+ create \"speeches_fixed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14131247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('raw_data/speeches.csv',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = data['speechContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac57b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_fixed = ['NA' if type(speech) == float else speech.replace('\\n', ' ').strip() for speech in speeches]\n",
    "speeches_fixed = [speech.replace('|', '') for speech in speeches_fixed if type(speech) != float]\n",
    "speech_dic = {'speechContentFix':speeches_fixed}\n",
    "speeches_fixed = pd.DataFrame(speech_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cec257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speechContent'] = speeches_fixed['speechContentFix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speeches = list(data['speechContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd19ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('raw_data/speeches_fixed.csv', encoding='utf-8', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9560d4",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "+ context of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a338750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#speechContent variable: create a list with each element as one speech content\n",
    "#by reading them into the file do the preprocessing:\n",
    "# delete the trailing spaces\n",
    "# put all the words in lowercase\n",
    "# lemmatize and tokenize the sentences (spaCy)\n",
    "\n",
    "#nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "with open('raw_data/speeches_fixed.csv', 'r', encoding='UTF-8') as file:\n",
    "    content = []\n",
    "    head = next(file)\n",
    "    #print(repr(head))\n",
    "    for line in file:\n",
    "        \n",
    "        # preparing the line string\n",
    "        line = next(file).strip().split(\";\")\n",
    "        \n",
    "        # preparing spacy doc object\n",
    "        speech = line[6].lower()\n",
    "        doc = nlp(speech)\n",
    "        \n",
    "        # lemmatization excluding stopwords, punctuations and numbers\n",
    "        lemm = [w.lemma_.lower() for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)]\n",
    "        \n",
    "        content.append(lemm)\n",
    "        \n",
    "# outcome: list of lists - each speech will be a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78524073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3d30a",
   "metadata": {},
   "source": [
    "+ training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a57a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(sentences=content, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model = Word2Vec(sentences=content, min_count=1, vector_size= 50, workers=3, window =3, sg = 1)\n",
    "\n",
    "#initialization parameter sg controls the mode. If True-ish (sg=1), skip-gram is used; if False-ish (sg=0), CBOW is used\n",
    "\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d22d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model \n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "    \n",
    "vector = model.wv.key_to_index['familiennachzug']  # get the index of the word in the model\n",
    "sims = model.wv.most_similar('familiennachzug', topn=30)  # get other similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02585023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
